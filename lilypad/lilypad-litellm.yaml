apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config-file
  namespace: lilypad
data:
  config.yaml: |
      model_list: 
        - model_name: gpt-3.5-turbo
          litellm_params:
            model: azure/gpt-turbo-small-ca
            api_base: https://my-endpoint-canada-berri992.openai.azure.com/
            api_key: os.environ/CA_AZURE_OPENAI_API_KEY
---
apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: litellm-secrets
data:
  STORE_MODEL_IN_DB: "True"
  LITELLM_MASTER_KEY: bWVvd19pbV9hX2NhdA==
  CA_AZURE_OPENAI_API_KEY: bWVvd19pbV9hX2NhdA== # your api key in base64
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: litellm-deployment
  namespace: lilypad
  annotations:
    argocd.argoproj.io/sync-wave: "2"
  labels:
    app: litellm
spec:
  selector:
    matchLabels:
      app: litellm
  template:
    metadata:
      labels:
        app: litellm
    spec:
      containers:
      - name: litellm
        image: ghcr.io/berriai/litellm:main-latest # it is recommended to fix a version generally
        ports:
        - containerPort: 4000
        volumeMounts:
        - name: config-volume
          mountPath: /app/proxy_server_config.yaml
          subPath: config.yaml
        resources:
          limits:
            memory: "512Mi"
            cpu: "500m"
          requests:
            memory: "256Mi"
            cpu: "250m"
        env:
        - name: LITELLM_LOG
          value: "DEBUG"
        - name: STORE_MODEL_IN_DB 
          value: "True"
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: services
              key: DATABASE_URL
        envFrom:
        - secretRef:
            name: litellm-secrets
      volumes:
        - name: config-volume
          configMap:
            name: litellm-config-file
---
apiVersion: v1
kind: Service
metadata:
  name: litellm-service
  namespace: lilypad
spec:
  type: NodePort
  selector:
    app: litellm
  ports:
    - protocol: TCP
      port: 4000
      targetPort: 4000